{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from disvae import init_specific_model, Trainer, Evaluator\n",
    "from disvae.utils.modelIO import save_model, load_model, load_metadata\n",
    "from disvae.models.losses import LOSSES, RECON_DIST, get_loss_f\n",
    "from disvae.models.vae import MODELS\n",
    "from utils.datasets import get_dataloaders, get_img_size, DATASETS\n",
    "from utils.helpers import (create_safe_directory, get_device, set_seed, get_n_param,\n",
    "                           get_config_section, update_namespace_, FormatterNoDuplicate)\n",
    "from utils.visualize import GifTraversalsTraining\n",
    "\n",
    "\n",
    "CONFIG_FILE = \"hyperparam.ini\"\n",
    "RES_DIR = \"results\"\n",
    "LOG_LEVELS = list(logging._levelToName.values())\n",
    "ADDITIONAL_EXP = ['custom', \"debug\", \"best_celeba\", \"best_dsprites\"]\n",
    "EXPERIMENTS = ADDITIONAL_EXP + [\"{}_{}\".format(loss, data)\n",
    "                                for loss in LOSSES\n",
    "                                for data in DATASETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(args_to_parse):\n",
    "    \"\"\"Parse the command line arguments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args_to_parse: list of str\n",
    "        Arguments to parse (splitted on whitespaces).\n",
    "    \"\"\"\n",
    "    default_config = get_config_section([CONFIG_FILE], \"Custom\")\n",
    "\n",
    "    description = \"PyTorch implementation and evaluation of disentangled Variational AutoEncoders and metrics.\"\n",
    "    parser = argparse.ArgumentParser(description=description,\n",
    "                                     formatter_class=FormatterNoDuplicate)\n",
    "\n",
    "    # General options\n",
    "    general = parser.add_argument_group('General options')\n",
    "    general.add_argument('name', type=str,\n",
    "                         help=\"Name of the model for storing and loading purposes.\")\n",
    "    general.add_argument('-L', '--log-level', help=\"Logging levels.\",\n",
    "                         default=default_config['log_level'], choices=LOG_LEVELS)\n",
    "    general.add_argument('--no-progress-bar', action='store_true',\n",
    "                         default=default_config['no_progress_bar'],\n",
    "                         help='Disables progress bar.')\n",
    "    general.add_argument('--no-cuda', action='store_true',\n",
    "                         default=default_config['no_cuda'],\n",
    "                         help='Disables CUDA training, even when have one.')\n",
    "    general.add_argument('-s', '--seed', type=int, default=default_config['seed'],\n",
    "                         help='Random seed. Can be `None` for stochastic behavior.')\n",
    "\n",
    "    # Learning options\n",
    "    training = parser.add_argument_group('Training specific options')\n",
    "    training.add_argument('--checkpoint-every',\n",
    "                          type=int, default=default_config['checkpoint_every'],\n",
    "                          help='Save a checkpoint of the trained model every n epoch.')\n",
    "    training.add_argument('-d', '--dataset', help=\"Path to training data.\",\n",
    "                          default=default_config['dataset'], choices=DATASETS)\n",
    "    training.add_argument('-x', '--experiment',\n",
    "                          default=default_config['experiment'], choices=EXPERIMENTS,\n",
    "                          help='Predefined experiments to run. If not `custom` this will overwrite some other arguments.')\n",
    "    training.add_argument('-e', '--epochs', type=int,\n",
    "                          default=default_config['epochs'],\n",
    "                          help='Maximum number of epochs to run for.')\n",
    "    training.add_argument('-b', '--batch-size', type=int,\n",
    "                          default=default_config['batch_size'],\n",
    "                          help='Batch size for training.')\n",
    "    training.add_argument('--lr', type=float, default=default_config['lr'],\n",
    "                          help='Learning rate.')\n",
    "\n",
    "    # Model Options\n",
    "    model = parser.add_argument_group('Model specfic options')\n",
    "    model.add_argument('-m', '--model-type',\n",
    "                       default=default_config['model'], choices=MODELS,\n",
    "                       help='Type of encoder and decoder to use.')\n",
    "    model.add_argument('-z', '--latent-dim', type=int,\n",
    "                       default=default_config['latent_dim'],\n",
    "                       help='Dimension of the latent variable.')\n",
    "    model.add_argument('-l', '--loss',\n",
    "                       default=default_config['loss'], choices=LOSSES,\n",
    "                       help=\"Type of VAE loss function to use.\")\n",
    "    model.add_argument('-r', '--rec-dist', default=default_config['rec_dist'],\n",
    "                       choices=RECON_DIST,\n",
    "                       help=\"Form of the likelihood ot use for each pixel.\")\n",
    "    model.add_argument('-a', '--reg-anneal', type=float,\n",
    "                       default=default_config['reg_anneal'],\n",
    "                       help=\"Number of annealing steps where gradually adding the regularisation. What is annealed is specific to each loss.\")\n",
    "\n",
    "    # Loss Specific Options\n",
    "    betaH = parser.add_argument_group('BetaH specific parameters')\n",
    "    betaH.add_argument('--betaH-B', type=float,\n",
    "                       default=default_config['betaH_B'],\n",
    "                       help=\"Weight of the KL (beta in the paper).\")\n",
    "\n",
    "    betaB = parser.add_argument_group('BetaB specific parameters')\n",
    "    betaB.add_argument('--betaB-initC', type=float,\n",
    "                       default=default_config['betaB_initC'],\n",
    "                       help=\"Starting annealed capacity.\")\n",
    "    betaB.add_argument('--betaB-finC', type=float,\n",
    "                       default=default_config['betaB_finC'],\n",
    "                       help=\"Final annealed capacity.\")\n",
    "    betaB.add_argument('--betaB-G', type=float,\n",
    "                       default=default_config['betaB_G'],\n",
    "                       help=\"Weight of the KL divergence term (gamma in the paper).\")\n",
    "\n",
    "    factor = parser.add_argument_group('factor VAE specific parameters')\n",
    "    factor.add_argument('--factor-G', type=float,\n",
    "                        default=default_config['factor_G'],\n",
    "                        help=\"Weight of the TC term (gamma in the paper).\")\n",
    "    factor.add_argument('--lr-disc', type=float,\n",
    "                        default=default_config['lr_disc'],\n",
    "                        help='Learning rate of the discriminator.')\n",
    "\n",
    "    btcvae = parser.add_argument_group('beta-tcvae specific parameters')\n",
    "    btcvae.add_argument('--btcvae-A', type=float,\n",
    "                        default=default_config['btcvae_A'],\n",
    "                        help=\"Weight of the MI term (alpha in the paper).\")\n",
    "    btcvae.add_argument('--btcvae-G', type=float,\n",
    "                        default=default_config['btcvae_G'],\n",
    "                        help=\"Weight of the dim-wise KL term (gamma in the paper).\")\n",
    "    btcvae.add_argument('--btcvae-B', type=float,\n",
    "                        default=default_config['btcvae_B'],\n",
    "                        help=\"Weight of the TC term (beta in the paper).\")\n",
    "\n",
    "    # Learning options\n",
    "    evaluation = parser.add_argument_group('Evaluation specific options')\n",
    "    evaluation.add_argument('--is-eval-only', action='store_true',\n",
    "                            default=default_config['is_eval_only'],\n",
    "                            help='Whether to only evaluate using precomputed model `name`.')\n",
    "    evaluation.add_argument('--is-metrics', action='store_true',\n",
    "                            default=default_config['is_metrics'],\n",
    "                            help=\"Whether to compute the disentangled metrcics. Currently only possible with `dsprites` as it is the only dataset with known true factors of variations.\")\n",
    "    evaluation.add_argument('--no-test', action='store_true',\n",
    "                            default=default_config['no_test'],\n",
    "                            help=\"Whether not to compute the test losses.`\")\n",
    "    evaluation.add_argument('--eval-batchsize', type=int,\n",
    "                            default=default_config['eval_batchsize'],\n",
    "                            help='Batch size for evaluation.')\n",
    "\n",
    "    args = parser.parse_args(args_to_parse)\n",
    "    if args.experiment != 'custom':\n",
    "        if args.experiment not in ADDITIONAL_EXP:\n",
    "            # update all common sections first\n",
    "            model, dataset = args.experiment.split(\"_\")\n",
    "            common_data = get_config_section([CONFIG_FILE], \"Common_{}\".format(dataset))\n",
    "            update_namespace_(args, common_data)\n",
    "            common_model = get_config_section([CONFIG_FILE], \"Common_{}\".format(model))\n",
    "            update_namespace_(args, common_model)\n",
    "\n",
    "        try:\n",
    "            experiments_config = get_config_section([CONFIG_FILE], args.experiment)\n",
    "            update_namespace_(args, experiments_config)\n",
    "        except KeyError as e:\n",
    "            if args.experiment in ADDITIONAL_EXP:\n",
    "                raise e  # only reraise if didn't use common section\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\"Main train and evaluation function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args: argparse.Namespace\n",
    "        Arguments\n",
    "    \"\"\"\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s - %(funcName)s: %(message)s',\n",
    "                                  \"%H:%M:%S\")\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(args.log_level.upper())\n",
    "    stream = logging.StreamHandler()\n",
    "    stream.setLevel(args.log_level.upper())\n",
    "    stream.setFormatter(formatter)\n",
    "    logger.addHandler(stream)\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    device = get_device(is_gpu=not args.no_cuda)\n",
    "    exp_dir = os.path.join(RES_DIR, args.name)\n",
    "    logger.info(\"Root directory for saving and loading experiments: {}\".format(exp_dir))\n",
    "\n",
    "    if not args.is_eval_only:\n",
    "\n",
    "        create_safe_directory(exp_dir, logger=logger)\n",
    "\n",
    "        if args.loss == \"factor\":\n",
    "            logger.info(\"FactorVae needs 2 batches per iteration. To replicate this behavior while being consistent, we double the batch size and the the number of epochs.\")\n",
    "            args.batch_size *= 2\n",
    "            args.epochs *= 2\n",
    "\n",
    "        # PREPARES DATA\n",
    "        train_loader = get_dataloaders(args.dataset,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       logger=logger)\n",
    "        logger.info(\"Train {} with {} samples\".format(args.dataset, len(train_loader.dataset)))\n",
    "\n",
    "        # PREPARES MODEL\n",
    "        args.img_size = get_img_size(args.dataset)  # stores for metadata\n",
    "        model = init_specific_model(args.model_type, args.img_size, args.latent_dim)\n",
    "        logger.info('Num parameters in model: {}'.format(get_n_param(model)))\n",
    "\n",
    "        # TRAINS\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "        model = model.to(device)  # make sure trainer and viz on same device\n",
    "        gif_visualizer = GifTraversalsTraining(model, args.dataset, exp_dir)\n",
    "        loss_f = get_loss_f(args.loss,\n",
    "                            n_data=len(train_loader.dataset),\n",
    "                            device=device,\n",
    "                            **vars(args))\n",
    "        trainer = Trainer(model, optimizer, loss_f,\n",
    "                          device=device,\n",
    "                          logger=logger,\n",
    "                          save_dir=exp_dir,\n",
    "                          is_progress_bar=not args.no_progress_bar,\n",
    "                          gif_visualizer=gif_visualizer)\n",
    "        trainer(train_loader,\n",
    "                epochs=args.epochs,\n",
    "                checkpoint_every=args.checkpoint_every,)\n",
    "\n",
    "        # SAVE MODEL AND EXPERIMENT INFORMATION\n",
    "        save_model(trainer.model, exp_dir, metadata=vars(args))\n",
    "\n",
    "    if args.is_metrics or not args.no_test:\n",
    "        model = load_model(exp_dir, is_gpu=not args.no_cuda)\n",
    "        metadata = load_metadata(exp_dir)\n",
    "        # TO-DO: currently uses train datatset\n",
    "        test_loader = get_dataloaders(metadata[\"dataset\"],\n",
    "                                      batch_size=args.eval_batchsize,\n",
    "                                      shuffle=False,\n",
    "                                      logger=logger)\n",
    "        loss_f = get_loss_f(args.loss,\n",
    "                            n_data=len(test_loader.dataset),\n",
    "                            device=device,\n",
    "                            **vars(args))\n",
    "        evaluator = Evaluator(model, loss_f,\n",
    "                              device=device,\n",
    "                              logger=logger,\n",
    "                              save_dir=exp_dir,\n",
    "                              is_progress_bar=not args.no_progress_bar)\n",
    "\n",
    "        evaluator(test_loader, is_metrics=args.is_metrics, is_losses=not args.no_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:14:43 INFO - main: Root directory for saving and loading experiments: results/VAE_dsprites\n",
      "17:14:43 INFO - main: Root directory for saving and loading experiments: results/VAE_dsprites\n",
      "17:14:49 INFO - __init__: Testing Device: cpu\n",
      "17:14:49 INFO - __init__: Testing Device: cpu\n",
      "17:14:49 INFO - __call__: Computing losses...\n",
      "17:14:49 INFO - __call__: Computing losses...\n",
      "17:14:50 INFO - __call__: Losses: {'recon_loss': 0.0054904380788001915, 'kl_loss': 0.03087637159559462, 'kl_loss_0': 0.00471328655232582, 'kl_loss_1': 0.003748927659135524, 'kl_loss_2': 0.004142963466282459, 'kl_loss_3': 0.0032982373948342754, 'kl_loss_4': 0.0043847971169283075, 'kl_loss_5': 0.0015813677614620384, 'kl_loss_6': 0.0008500522068199427, 'kl_loss_7': 0.0017157198613898218, 'kl_loss_8': 0.00422867979137555, 'kl_loss_9': 0.002212339300450271, 'loss': 0.3053871506274877}\n",
      "17:14:50 INFO - __call__: Losses: {'recon_loss': 0.0054904380788001915, 'kl_loss': 0.03087637159559462, 'kl_loss_0': 0.00471328655232582, 'kl_loss_1': 0.003748927659135524, 'kl_loss_2': 0.004142963466282459, 'kl_loss_3': 0.0032982373948342754, 'kl_loss_4': 0.0043847971169283075, 'kl_loss_5': 0.0015813677614620384, 'kl_loss_6': 0.0008500522068199427, 'kl_loss_7': 0.0017157198613898218, 'kl_loss_8': 0.00422867979137555, 'kl_loss_9': 0.002212339300450271, 'loss': 0.3053871506274877}\n",
      "17:14:50 INFO - __call__: Finished evaluating after 0.0 min.\n",
      "17:14:50 INFO - __call__: Finished evaluating after 0.0 min.\n"
     ]
    }
   ],
   "source": [
    "args = parse_arguments(\"--is-eval-only VAE_dsprites\".split(\" \"))\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
